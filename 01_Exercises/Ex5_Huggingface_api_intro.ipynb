{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22a9dd2",
   "metadata": {},
   "source": [
    "# ðŸ¤— Introduction to Hugging Face Transformers API\n",
    "\n",
    "This notebook introduces the core components of the Hugging Face Transformers library: **tokenizers**, **models**, and **pipelines**. It provides hands-on examples to help you understand how to work with pre-trained models before fine-tuning them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f80b09",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbad324",
   "metadata": {},
   "source": [
    "## 2. Load a Pre-trained Tokenizer\n",
    "\n",
    "The tokenizer breaks text into tokens and converts them into input IDs that the model can understand.\n",
    "\n",
    "- The tokenizer and model must be compatible (i.e., from the same model family).\n",
    "    * The tokenizer acts like a lookup dictionary: it breaks input text into tokens and maps them to numerical IDs the model understands.\n",
    "    * Each model is trained using a specific tokenizer, so using a different one can result in mismatched input â€” leading to errors, poor output, or model failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b40974",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Example text\n",
    "text = \"To be, or not to be, that is the question.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer(text)\n",
    "print(\"Token IDs:\", tokens[\"input_ids\"])\n",
    "print(\"Decoded back:\", tokenizer.decode(tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858e49e",
   "metadata": {},
   "source": [
    "## 3. Load a Pre-trained Model\n",
    "\n",
    "We now load the GPT-2 language model to process the tokenized input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Convert input to tensor\n",
    "input_ids = torch.tensor([tokens[\"input_ids\"]])\n",
    "\n",
    "# Generate output from model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "print(\"Model output shape:\", outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56947b64",
   "metadata": {},
   "source": [
    "## 4. Use the Pipeline for Text Generation\n",
    "\n",
    "Hugging Face's `pipeline` abstraction simplifies using models for common tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b98961",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"To be, or not to be, that is the question.\"\n",
    "generated = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated text:\\n\")\n",
    "print(generated[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3eae64",
   "metadata": {},
   "source": [
    "## Additional Concepts to Know\n",
    "\n",
    "Here are some key ideas and best practices when working with Hugging Face Transformers:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Always Check the Model Card on Hugging Face Hub\n",
    "- Every model on [https://huggingface.co/models](https://huggingface.co/models) has a **Model Card**.\n",
    "- It tells you:\n",
    "  - What the model was trained on\n",
    "  - Supported tasks\n",
    "  - Limitations and licenses\n",
    "  - Example usage\n",
    "\n",
    "---\n",
    "\n",
    "### 2.  `AutoModel` vs `AutoModelFor...`\n",
    "- `AutoModel` gives you the **base model** that outputs raw hidden states (useful for feature extraction, embeddings).\n",
    "- `AutoModelForCausalLM`, `AutoModelForSequenceClassification`, etc., include **task-specific heads**.\n",
    "  - âœ… Example: `AutoModelForCausalLM` adds a **language modeling** head on top of GPT-2.\n",
    "  \n",
    "```python\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")  # base model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `AutoTokenizer`\n",
    "- Automatically loads the right tokenizer for a given model.\n",
    "- Handles:\n",
    "  - Lowercasing (if needed)\n",
    "  - Byte Pair Encoding (BPE), WordPiece, SentencePiece, etc.\n",
    "  - Padding, truncation, special tokens\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `pipeline`: Easy Inference for Common Tasks\n",
    "- Abstracts away model/tokenizer logic.\n",
    "- Great for:\n",
    "  - `text-generation`\n",
    "  - `sentiment-analysis`\n",
    "  - `translation`\n",
    "  - `question-answering`\n",
    "  \n",
    "```python\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "print(summarizer(\"Long text goes here...\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "These tools help you work quickly, but you can also drop into low-level APIs for full control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1044eea",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "- **Tokenizer**: Converts raw text into tokens and input IDs.\n",
    "- **Model**: Processes input IDs to produce predictions.\n",
    "- **Pipeline**: A high-level interface for common tasks like text generation.\n",
    "\n",
    "These components are the foundation for working with Hugging Face models, and you'll use them again when fine-tuning on your own datasets (e.g., Shakespeare)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
