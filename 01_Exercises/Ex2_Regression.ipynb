{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "## Problem 2.1: regression\n",
    "\n",
    "In this excersise we train a simple neural network to approximate a noisy sine function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed\n",
    "\n",
    "Needed for reproducibility of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions to generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoid(x):\n",
    "  return np.sin(3 * np.pi * x[..., 0])\n",
    "\n",
    "def generate_1d_data(num_training_points, observation_noise_variance):\n",
    "  \"\"\"Generate noisy sinusoidal observations at a random set of points.\n",
    "\n",
    "  Returns:\n",
    "     observation_index_points, observations\n",
    "  \"\"\"\n",
    "  index_points_ = np.random.uniform(-1., 1., (num_training_points, 1))\n",
    "  index_points_ = index_points_.astype(np.float64)\n",
    "  # y = f(x) + noise\n",
    "  observations_ = (sinusoid(index_points_) +\n",
    "                   np.random.normal(loc=0,\n",
    "                                    scale=np.sqrt(observation_noise_variance),\n",
    "                                    size=(num_training_points)))\n",
    "  return index_points_, observations_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAINING_POINTS = 100\n",
    "x, y = generate_1d_data(num_training_points=NUM_TRAINING_POINTS, observation_noise_variance=.1)\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "y = y[:,None]\n",
    "\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize parameters for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "learning_rate = 0.01 \n",
    "input_size = x.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions\n",
    "Takes `torch.nn.Module` as an input and applies the specified weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(-0.01, 0.01)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hook function to store hidden layer outputs in the `hidden_outputs` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_outputs = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    hidden_outputs.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a neural network \n",
    "\n",
    "Let's call it `SineModel`, since we are going to approximate a sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SineModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  \n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set number of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 5     # dont' change it unless you know what you are doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define empty array to save the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_tr = np.empty(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SineModel(input_size, hidden_size)\n",
    "\n",
    "# Register hook to the hidden layer\n",
    "model.fc1.register_forward_hook(hook_fn)\n",
    "\n",
    "# Apply the specified weight initialization.\n",
    "model.apply(weights_init_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss and optimizer\n",
    "For this regression problem we use the mean-square-error:\n",
    "$$J(\\mathbf{w}) = \\frac{1}{n}\\sum\\limits_{i=1}^n(\\hat{y}_i - y_i)^2$$\n",
    "which is available in `pytorch` as `nn.MSELoss`.\n",
    "\n",
    "As an optimizer we suggest to use `Adam` - stochastic gradient-based optimization method. See the [arxive paper](https://arxiv.org/abs/1412.6980) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "criterion = nn.MSELoss() \n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run training of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Get our predictions\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    # Save loss to the array for further visualization\n",
    "    e_tr[epoch] = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the training loss\n",
    "- How does the loss change with training epoch?\n",
    "- What does it mean? What does it say about quality of approximation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.plot(e_tr, label='Training')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the approximation result\n",
    "- Evaluate quality: is it a good fit? Why?\n",
    "- Change the **number of training epochs** in the line `num_epochs = 5000`, reinitialize the model and run the training again (the easiest way is to run all the cells (Run -> Run all cells)).\n",
    "- How does the number of training epochs influence the training loss and the approximation quality? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.scatter(x.detach().numpy(), y_pred.detach().numpy(), color='r', marker='s', label='Prediction')\n",
    "plt.scatter(x.detach().numpy(), y.detach().numpy(), label='Ground truth')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the hidden units\n",
    "\n",
    "- Change the random seed to vary the weights initialization.  How does it influence the result?\n",
    "- How do the values of hidden units (bottom plot) relate to the approximated function? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last hidden output\n",
    "last_output = hidden_outputs[-1]\n",
    "\n",
    "# get weights for the last hidden layer\n",
    "w = model.fc2.weight.detach().numpy()[0]\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "ax[0].scatter(x.detach().numpy(), y_pred.detach().numpy(), color='r', marker='s', label='Prediction')\n",
    "ax[0].scatter(x.detach().numpy(), y.detach().numpy(), label='Ground truth')\n",
    "ax[0].set_ylabel(r'$y$')\n",
    "ax[0].legend()\n",
    "\n",
    "colors = ['k','b','y','m','g']\n",
    "for i, c in enumerate(colors):\n",
    "    label = \"Hidden Output {num}\"\n",
    "    ax[1].scatter(x.detach().numpy(), torch.sigmoid(last_output[:,i]).detach().numpy()*w[i], color=c, label=label.format(num=i))\n",
    "ax[1].legend()\n",
    "ax[1].set_ylabel(r'$h(x)\\cdot w$')\n",
    "plt.xlabel(r'$x$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove or comment the line below to run the complete notebook via Run->Run all cells\n",
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.2: gradient descent\n",
    "\n",
    "In this excersise we train a simplified neural network to approximate the same noisy sine function as in a previous excersise. We will observe the work of the gradient descent by visualizing weights during the training and loss function. To enable this visualization, we have reduced the number of weights to train upto two: $w_1$ and $w_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "learning_rate = 0.01 \n",
    "input_size = x.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the new simplified neural network\n",
    "\n",
    "Let's call it `SimpleModel`, since it has only 2 parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, bias=False) \n",
    "        self.fc2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define empty arrays to save the data required for the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights during the training\n",
    "w1_list = np.empty(num_epochs)\n",
    "w2_list = np.empty(num_epochs)\n",
    "\n",
    "# training loss vs. epoch\n",
    "e_tr = np.empty(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "In this excersise we fix the weights if the layer `fc2` and train only the weights of the layer `fc1`. Statement `required_grad=False` tells pytorch do not update the weights in the given layer. We also set the number of hidden units strictly to 2 to enable visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor with fixed weights\n",
    "weights_fc2 = torch.tensor([[1.0], [-1.0]], dtype=torch.float)\n",
    "\n",
    "# create the model\n",
    "simple_model = SimpleModel(input_size, 2)\n",
    "simple_model.fc2.weight = nn.Parameter(weights_fc2.T, requires_grad=False)\n",
    "\n",
    "# check whether the weights have been set correctly\n",
    "print(simple_model.fc2.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task here**. Check the output: are the weights havee been set correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and save the loss function in the given range\n",
    "\n",
    "We need it for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=100\n",
    "w0 = np.linspace (-2, 2, num=num)\n",
    "w1 = np.linspace (-2, 2, num=num)\n",
    "xx, yy = np.meshgrid (w0, w1)\n",
    "z = np.zeros((num, num))\n",
    "\n",
    "criterion = nn.MSELoss() # negative log likelihood loss\n",
    "    \n",
    "for i in range(num):\n",
    "    for j in range(num):\n",
    "        simple_model.fc1.weight = nn.Parameter(torch.tensor([[xx[i, j]], [yy[i, j]]], dtype=torch.float), requires_grad=True)\n",
    "        y_pred = simple_model(x)\n",
    "        z[i, j] = criterion(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the simplified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and set the initial weights for the layer fc1\n",
    "weights_fc1 = torch.tensor([[-2.0], [2.0]], dtype=torch.float)\n",
    "simple_model.fc1.weight = nn.Parameter(weights_fc1, requires_grad=True)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(simple_model.parameters(), lr=learning_rate) \n",
    "\n",
    "# run training\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Get our predictions\n",
    "    y_pred = simple_model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    # save the training loss\n",
    "    e_tr[epoch] = loss.detach().numpy()\n",
    "\n",
    "    # save the weights for visualization\n",
    "    w1_list[epoch] = simple_model.fc1.weight[0].detach().numpy()[0]\n",
    "    w2_list[epoch] = simple_model.fc1.weight[1].detach().numpy()[0]\n",
    "\n",
    "    # uncomment the line below if you want to print the saved weights and loss\n",
    "    #print(\"Epoch: {epoch}, Loss: {loss}, W1: {w1}, W2: {w2}\".format(epoch=epoch, loss=e_tr[epoch], w1=w1_list[epoch], w2=w2_list[epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the gradient descent\n",
    "\n",
    "- Change the **learning rate** in the line `learning_rate = 0.01`, reinitialize model and optimizer and run the training again (just run all the cells of the present excersise).\n",
    "- How does the learning rate influence the training loss and the approximation quality? Why? Does it influence anything else? What?\n",
    "- Vary the initial values of the weights in the line `weights_fc1 = torch.tensor([[-2.0], [2.0]], dtype=torch.float)` in the block above. How do they influence the behaviour of the gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(projection = '3d')\n",
    "\n",
    "ax.plot_surface(xx, yy, z, rstride=5, cstride=5, cmap=cm.coolwarm, linewidth=1, antialiased=True, edgecolor='none', alpha=0.75)\n",
    "ax.plot(w1_list, w2_list, e_tr, 'ko', linewidth=5)\n",
    "\n",
    "# uncomment the two lines below and change the angles if you want to rotate the plot\n",
    "#angles = (0, 0, 0)\n",
    "#ax.view_init(elev=angles[0], azim=angles[1], roll=angles[2])\n",
    "\n",
    "plt.xlabel(r\"$w_0$\", fontsize=14)\n",
    "plt.ylabel(r\"$w_1$\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
