{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1a1437",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 on Shakespeare Text\n",
    "\n",
    "This tutorial will guide you through fine-tuning a GPT-2 Huggingface model on a Shakespeare text dataset. We'll:\n",
    "\n",
    "1. Load and preprocess the text data from a `.txt` file.\n",
    "2. Tokenize the data.\n",
    "3. Set up the Trainer.\n",
    "4. Fine-tune GPT-2.\n",
    "5. Save the fine-tuned model.\n",
    "6. Load the fine-tuned model. \n",
    "7. Generate text from the fine-tuned model ala Shakespearean Text\n",
    "\n",
    "Run each cell in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9626dc",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Ensure you have the necessary libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73dd6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets scikit-learn torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509c631",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Shakespeare Text\n",
    "\n",
    "### ✅ Goal\n",
    "\n",
    "Split the text like this:\n",
    "\n",
    "```\n",
    "Citizen:\n",
    "This, here before you.\n",
    "CORIOLANUS:\n",
    "Thank you, sir: farewell.\n",
    "O world, thy slippery turns! ...\n",
    "```\n",
    "\n",
    "Into structured chunks like:\n",
    "\n",
    "- `Citizen: This, here before you.`\n",
    "- `CORIOLANUS: Thank you, sir: farewell. O world, thy slippery turns! ...`\n",
    "\n",
    "Each of these becomes one training sample.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Step-by-Step Breakdown\n",
    "\n",
    "#### 1. **Speaker Format Detection**\n",
    "\n",
    "We detect lines that look like:\n",
    "\n",
    "```\n",
    "CHARACTER_NAME:\n",
    "```\n",
    "\n",
    "These always start in uppercase and end with a colon.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Regex Pattern Used**\n",
    "\n",
    "```python\n",
    "pattern = r'(?=^[A-Z][A-Za-z\\' ]+-?:\\n)'\n",
    "```\n",
    "\n",
    "This matches:\n",
    "- Names with spaces (`First Servingman:`)\n",
    "- Apostrophes (`O'Conner:`)\n",
    "- Optional dashes (`Stage-Direction:`)\n",
    "- One or more capital letters followed by a colon and newline\n",
    "\n",
    "The `(?=...)` syntax is a **lookahead**, meaning it **splits the text just before** the speaker line, without removing it.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Splitting the Text**\n",
    "\n",
    "We use:\n",
    "\n",
    "```python\n",
    "re.split(pattern, text, flags=re.MULTILINE)\n",
    "```\n",
    "\n",
    "This breaks the raw text into chunks, one per speaker.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Cleaning and Reattaching**\n",
    "\n",
    "The split removes the speaker line, so we use `re.findall(...)` to capture the speaker names and reattach them.\n",
    "\n",
    "Each block is also flattened (newlines → spaces) using:\n",
    "\n",
    "```python\n",
    "part.replace('\\n', ' ').strip()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Result\n",
    "\n",
    "Each item in the final list looks like:\n",
    "\n",
    "```python\n",
    "\"CORIOLANUS: Thank you, sir: farewell. O world, thy slippery turns! ...\"\n",
    "```\n",
    "\n",
    "This structure is ideal for training GPT models on realistic, stylized dialogue data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_by_speakers(text):\n",
    "    # Corrected character class: dash is moved to the end so it's not interpreted as a range\n",
    "    pattern = r'(?=^[A-Z][A-Za-z\\' ]+-?:\\n)'  # Match lines like \"First Servingman:\\n\"\n",
    "    \n",
    "    # Split on speaker lines\n",
    "    parts = re.split(pattern, text, flags=re.MULTILINE)\n",
    "\n",
    "    # Find the matching speaker headers to reattach\n",
    "    speaker_lines = re.findall(pattern, text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove any empty or header-only sections\n",
    "    parts = [p for p in parts if p.strip()]\n",
    "    full_blocks = [f\"{speaker.strip()} {part.replace('\\\\n', ' ').strip()}\"\n",
    "                   for speaker, part in zip(speaker_lines, parts)]\n",
    "\n",
    "    return full_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_shakespeare_blocks(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "    cleaned_blocks = split_by_speakers(raw_text)\n",
    "    return cleaned_blocks\n",
    "\n",
    "# Load data\n",
    "file_path = \"./data/input.txt\"  # Make sure the file is in the working directory\n",
    "text_blocks = load_shakespeare_blocks(file_path)\n",
    "\n",
    "# Create a Dataset\n",
    "dataset = Dataset.from_dict({\"text\": text_blocks})\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42) # define 20% test dataset\n",
    "\n",
    "dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601af132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4e66e",
   "metadata": {},
   "source": [
    "## 3. Load GPT-2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ffc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 pad token-> for padding in the tokenizer if the text is less than the model input length.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8ed3f",
   "metadata": {},
   "source": [
    "## 4. Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = tokenizer(\n",
    "        text[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(tokenize, remove_columns=[\"text\"])\n",
    "tokenized_test = dataset[\"test\"].map(tokenize, remove_columns=[\"text\"])\n",
    "\n",
    "tokenized_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ebe621",
   "metadata": {},
   "source": [
    "## 5. Set Up Data Collator and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31499ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-shakespeare-finetuned\",\n",
    "    num_train_epochs=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594c04f",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a937b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc786a",
   "metadata": {},
   "source": [
    "## 7. Save the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gpt2-shakespeare-finetuned\")\n",
    "tokenizer.save_pretrained(\"gpt2-shakespeare-finetuned\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a7de3",
   "metadata": {},
   "source": [
    "## 8. Load the Fine-Tuned Model for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deea438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_path = './gpt2-shakespeare-finetuned' # Wind the folder where the fine-tuned shakespearean model is located\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)#  We can use AutoTokenizer to detect model tokenizer automatically, i.e., add model_path in its parameter.\n",
    "model =  AutoModelForCausalLM.from_pretrained(model_path) # AutoModelFor..., add model_path is its parameter\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab708e",
   "metadata": {},
   "source": [
    "## 9. Generate Shakespearean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41367e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a prompt to the model\n",
    "prompt = \"To be, or not to be\"\n",
    "results = generator(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "print(\"Generated Text:\\n\")\n",
    "print(results[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
