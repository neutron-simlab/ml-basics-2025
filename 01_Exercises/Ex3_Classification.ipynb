{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "## Problem 3.1: binary classification\n",
    "- Train the given binary classification model\n",
    "- Evaluate the model performance\n",
    "- Evaluate the classification quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "x=np.load('data/features_15_18.npy') \n",
    "t=np.load('data/labels_15_18.npy')\n",
    "\n",
    "x=x.T\n",
    "t = t[:,None]\n",
    "\n",
    "# set labels to 0 and 1\n",
    "for i,label in enumerate(np.unique(t)):\n",
    "    t[t == label] = i\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "indices = np.random.permutation(x.shape[0])\n",
    "n = len(indices)\n",
    "\n",
    "# split\n",
    "training_idx, test_idx, val_idx = indices[:round(0.6*n)], indices[round(0.6*n):round(0.8*n)], indices[round(0.8*n):]\n",
    "x_tr, x_te, x_vl  = x[training_idx,:], x[test_idx,:], x[val_idx,:]\n",
    "t_tr, t_te, t_vl  = t[training_idx,:], t[test_idx,:], t[val_idx,:]\n",
    "\n",
    "# convert to pytorch tensors\n",
    "xtr = torch.tensor(x_tr, dtype=torch.float)\n",
    "xte = torch.tensor(x_te, dtype=torch.float)\n",
    "xvl = torch.tensor(x_vl, dtype=torch.float)\n",
    "ttr = torch.tensor(t_tr, dtype=torch.float)\n",
    "tte = torch.tensor(t_te, dtype=torch.float)\n",
    "tvl = torch.tensor(t_vl, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize parameters for learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000  \n",
    "learning_rate = 5e-3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper function\n",
    "Takes `torch.nn.Module` as an input and applies the specified weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(-0.01, 0.01)\n",
    "        m.bias.data.fill_(0)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set number of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1   # make changes here to increase the model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define empty arrays to save the performance evaluation measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_tr = np.empty(num_epochs)   # training loss\n",
    "e_te = np.empty(num_epochs)   # test loss\n",
    "e_vl = np.empty(num_epochs)   # validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model: define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = xtr.shape[1] \n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(hidden_size, 1),\n",
    "                      nn.Sigmoid())\n",
    "\n",
    "model.apply(weights_init_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss and optimizer\n",
    "\n",
    "For the binary classification we use the cross-entropy loss function:\n",
    "\n",
    "$$J(\\mathbf{w}) = -\\frac{1}{n}\\sum\\limits_{i=1}^n\\left[y_i\\log q_i + (1 - y_i)\\log(1-q_i)\\right]$$\n",
    "\n",
    "which is available in `pytorch` as `nn.BCELoss`.\n",
    "\n",
    "As an optimizer we suggest to use `Adam` - stochastic gradient-based optimization method. See the [arxive paper](https://arxiv.org/abs/1412.6980) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run training of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Get our predictions\n",
    "    otr = model(xtr)\n",
    "    \n",
    "    loss = criterion(otr, ttr)\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "  \n",
    "    # save the performance evaluation measures\n",
    "    e_tr[epoch] = loss\n",
    "    e_te[epoch] = criterion(model(xte), tte)\n",
    "    e_vl[epoch] = criterion(model(xvl), tvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training performance evaluation measures\n",
    "\n",
    "Run the code below and evaluate the resulting plot:\n",
    "- How Training/Test/Validation loss changes with the epoch?\n",
    "- Which loss is higher in the beginning of the training?\n",
    "- Which loss is higher at the end of the training?\n",
    "- What does it tell you about performance of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.plot(e_tr, label='Training')\n",
    "plt.plot(e_te, label='Test')\n",
    "plt.plot(e_vl, label='Validation')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary cross entropy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the classification result\n",
    "\n",
    "Run the code below to visualize the classification result. \n",
    "- How do you evaluate the quality of classification? Why?\n",
    "- What should be done to improve the quality of classification? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.arange(-100, 100, 1)\n",
    "x2 = np.arange(-100, 100, 1)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "X = torch.tensor(np.vstack((X1.flatten(),X2.flatten())).T, dtype=torch.float)\n",
    "Z =  model(X).reshape(X1.shape).detach().numpy()\n",
    "\n",
    "colormap = cm.bwr\n",
    "scalarMap = cm.ScalarMappable(norm=Normalize(vmin=Z.min(), vmax=Z.max()), cmap=colormap)\n",
    "\n",
    "Z_colored = scalarMap.to_rgba(Z)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(projection = '3d')\n",
    "np.zeros(X1.shape)\n",
    "ax.scatter(x_tr[t_tr[:,0]==0,0],x_tr[t_tr[:,0]==0,1])\n",
    "ax.scatter(x_tr[t_tr[:,0]==1,0],x_tr[t_tr[:,0]==1,1])\n",
    "surf = ax.plot_surface(X1, X2, -np.ones(X1.shape), rstride=1, cstride=1, facecolors=Z_colored)\n",
    "ax.view_init(90, 0)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.2: model complexity\n",
    "\n",
    "You can increase the model complexity by increasing the number of hidden units.\n",
    "- Scroll to the line `hidden_size = 1`\n",
    "- Set `hidden_size` equal to 3, 5, 7, 9 or other integer value between 1 and 10\n",
    "- Run all the cells (Run -> Run all cells) to train the neural network with the given number of hidden units\n",
    "- Evaluate the model performance: what do you observe? Why? What does it tell about your model?\n",
    "- Visualize the classification result. How do you evaluate the classification quality? Why?\n",
    "- What happens to the neural network when you increase the number of hidden units? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
